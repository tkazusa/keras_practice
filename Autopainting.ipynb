{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autopainting.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tkazusa/keras_practice/blob/master/Autopainting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "0Fy4XEXkRxly",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Auto painting with CAE"
      ]
    },
    {
      "metadata": {
        "id": "9zP2rIuKRxl3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "219972dc-bf4b-4f30-c8f7-7b120b44813c"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import math\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Input, MaxPooling2D, UpSampling2D, Lambda\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img, ImageDataGenerator"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2suQPvLKRxmH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### データの読み込み"
      ]
    },
    {
      "metadata": {
        "id": "1IUGTk4ARxmJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_path = 'PATH/TO/DIR'\n",
        "data_lists = glob.glob(os.path.join(data_path, '*.jpg'))\n",
        "\n",
        "val_n_sample = math.floor(len(data_lists)*0.1)\n",
        "test_n_sample = math.floor(len(data_lists)*0.1)\n",
        "train_n_sample = len(data_lists) - val_n_sample - test_n_sample\n",
        "\n",
        "val_lists = data_lists[:val_n_sample]\n",
        "test_lists = data_lists[val_n_sample:val_n_sample + test_n_sample]\n",
        "train_lists = data_lists[val_n_sample + test_n_sample:train_n_sample + val_n_sample + test_n_sample]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DW3HX3JkRxmR",
        "colab_type": "code",
        "colab": {},
        "outputId": "5ad8c759-786e-4a68-b000-510e7e194afa"
      },
      "cell_type": "code",
      "source": [
        "print(len(train_lists), len(val_lists), len(test_lists))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23464 2932 2932\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p2BxFm2eRxmZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###  前処理：「RGB」を「LAB」に変換"
      ]
    },
    {
      "metadata": {
        "id": "P0GkHRuJRxmc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "\n",
        "img_size = 224\n",
        "def rgb2lab(rgb):\n",
        "    assert rgb.dtype == 'uint8'\n",
        "    return cv2.cvtColor(rgb, cv2.COLOR_RGB2Lab)\n",
        "\n",
        "def lab2rgb(lab):\n",
        "    assert lab.dtype == 'uint8'\n",
        "    return cv2.cvtColor(lab, cv2.COLOR_Lab2RGB)\n",
        "\n",
        "def get_lab_from_data_list(data_list):\n",
        "    x_lab = []\n",
        "    for f in data_list:\n",
        "        rgb = img_to_array(\n",
        "            load_img(\n",
        "                f, \n",
        "                target_size=(img_size, img_size)\n",
        "            )\n",
        "        ).astype(np.uint8)\n",
        "        lab = rgb2lab(rgb)\n",
        "        x_lab.append(lab)\n",
        "    return np.stack(x_lab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7khqMMEvRxmj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### モデルの構築"
      ]
    },
    {
      "metadata": {
        "id": "uhwVzSnXRxml",
        "colab_type": "code",
        "colab": {},
        "outputId": "676738ba-d295-4728-b0bb-5db9129b2d29"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.layers import Conv2DTranspose\n",
        "\n",
        "def build_autoencoder()\n",
        "  autoencoder = Sequential()\n",
        "  # Encoder\n",
        "  autoencoder.add(Conv2D(32, (3, 3),  (1, 1),\n",
        "                         activation='relu', padding='same',\n",
        "                         input_shape=(224, 224, 1)))\n",
        "  autoencoder.add(Conv2D(64, (3, 3), (2, 2), activation='relu', padding='same'))\n",
        "  autoencoder.add(Conv2D(128, (3, 3), (2, 2), activation='relu', padding='same'))\n",
        "  autoencoder.add(Conv2D(256, (3, 3), (2, 2), activation='relu', padding='same'))\n",
        "\n",
        "  # Decoder\n",
        "  autoencoder.add(Conv2DTranspose(128, (3, 3), (2, 2), activation='relu', padding='same'))\n",
        "  autoencoder.add(Conv2DTranspose(64, (3, 3), (2, 2), activation='relu', padding='same'))\n",
        "  autoencoder.add(Conv2DTranspose(32, (3, 3), (2, 2), activation='relu', padding='same'))\n",
        "  autoencoder.add(Conv2D(2, (1, 1), (1, 1), activation='relu', padding='same'))\n",
        "\n",
        "  autoencoder.compile(optimizer='adam', loss='mse')\n",
        "  return autoencoder\n",
        "\n",
        "autoencoder = build_autoencoder()\n",
        "autoencoder.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/backend.py:1557: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 224, 224, 32)      320       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 112, 112, 64)      18496     \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 56, 56, 128)       73856     \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 28, 28, 256)       295168    \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 56, 56, 128)       295040    \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 112, 112, 64)      73792     \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTr (None, 224, 224, 32)      18464     \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 224, 224, 2)       66        \n",
            "=================================================================\n",
            "Total params: 775,202\n",
            "Trainable params: 775,202\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ROcnGLGbRxnN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### ジェネレータ関数の定義"
      ]
    },
    {
      "metadata": {
        "id": "pF806dVcRxnP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generator_with_preprocessing(data_list, batch_size, shuffle=False):\n",
        "    while True:\n",
        "        if shuffle:\n",
        "            np.random.shuffle(data_list)\n",
        "        for i in range(0, len(data_list), batch_size):\n",
        "            batch_list = data_list[i:i + batch_size]\n",
        "            batch_lab = get_lab_from_data_list(batch_list)\n",
        "            batch_l = batch_lab[:, :, :, 0:1]\n",
        "            batch_ab = batch_lab[:, :, :, 1:]\n",
        "            yield (batch_l, batch_ab)\n",
        "            \n",
        "batch_size = 30\n",
        "\n",
        "train_gen = generator_with_preprocessing(train_lists, batch_size, shuffle=True)\n",
        "val_gen = generator_with_preprocessing(val_lists, batch_size)\n",
        "test_gen = generator_with_preprocessing(test_lists, batch_size)\n",
        "\n",
        "train_steps = math.ceil(len(train_lists)/batch_size)\n",
        "val_steps = math.ceil(len(val_lists)/batch_size)\n",
        "test_steps = math.ceil(len(test_lists)/batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AtyY8ZmsRxpY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### モデルの学習"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "iFU1IUFoRxpc",
        "colab_type": "code",
        "colab": {},
        "outputId": "c1fd0230-a7cf-4dae-a1b8-103c4cccc81f"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "valid_ck = ModelCheckpoint('{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "callbacks = [valid_ck]\n",
        "\n",
        "epochs= 150\n",
        "    \n",
        "autoencoder.fit_generator(\n",
        "    generator=train_gen,\n",
        "    steps_per_epoch=train_steps,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_gen,\n",
        "    validation_steps=val_steps,\n",
        "    # callbacks=callbacks, # コールバックを使用する場合はコメントアウトを外す\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "783/783 [==============================]783/783 [==============================] - 354s 452ms/step - loss: 2423.3882 - val_loss: 1638.8180\n",
            "\n",
            "Epoch 2/150\n",
            "783/783 [==============================]783/783 [==============================] - 350s 447ms/step - loss: 457.8891 - val_loss: 266.4778\n",
            "\n",
            "Epoch 3/150\n",
            "783/783 [==============================]783/783 [==============================] - 350s 447ms/step - loss: 280.2091 - val_loss: 267.3231\n",
            "\n",
            "Epoch 4/150\n",
            "783/783 [==============================]783/783 [==============================] - 350s 447ms/step - loss: 277.8285 - val_loss: 309.7160\n",
            "\n",
            "Epoch 5/150\n",
            "783/783 [==============================]783/783 [==============================] - 348s 445ms/step - loss: 280.3955 - val_loss: 259.7945\n",
            "\n",
            "Epoch 6/150\n",
            "783/783 [==============================]783/783 [==============================] - 348s 445ms/step - loss: 279.2827 - val_loss: 258.8755\n",
            "\n",
            "Epoch 7/150\n",
            "783/783 [==============================]783/783 [==============================] - 349s 446ms/step - loss: 280.4658 - val_loss: 264.9794\n",
            "\n",
            "Epoch 8/150\n",
            "783/783 [==============================]783/783 [==============================] - 350s 447ms/step - loss: 280.0380 - val_loss: 274.2946\n",
            "\n",
            "Epoch 9/150\n",
            "783/783 [==============================]783/783 [==============================] - 349s 446ms/step - loss: 279.4921 - val_loss: 258.4362\n",
            "\n",
            "Epoch 10/150\n",
            "783/783 [==============================]783/783 [==============================] - 348s 445ms/step - loss: 281.2955 - val_loss: 264.4749\n",
            "\n",
            "Epoch 11/150\n",
            "783/783 [==============================]783/783 [==============================] - 347s 443ms/step - loss: 280.8688 - val_loss: 263.2390\n",
            "\n",
            "Epoch 12/150\n",
            "783/783 [==============================]783/783 [==============================] - 346s 442ms/step - loss: 276.0020 - val_loss: 263.4226\n",
            "\n",
            "Epoch 13/150\n",
            "783/783 [==============================]783/783 [==============================] - 347s 443ms/step - loss: 269.1989 - val_loss: 294.1512\n",
            "\n",
            "Epoch 14/150\n",
            "783/783 [==============================]783/783 [==============================] - 345s 440ms/step - loss: 282.5054 - val_loss: 343.7505\n",
            "\n",
            "Epoch 15/150\n",
            "783/783 [==============================]783/783 [==============================] - 344s 440ms/step - loss: 279.3741 - val_loss: 280.5127\n",
            "\n",
            "Epoch 16/150\n",
            "783/783 [==============================]783/783 [==============================] - 344s 439ms/step - loss: 280.4771 - val_loss: 269.1592\n",
            "\n",
            "Epoch 17/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 439ms/step - loss: 279.5777 - val_loss: 285.7405\n",
            "\n",
            "Epoch 18/150\n",
            "783/783 [==============================]783/783 [==============================] - 344s 439ms/step - loss: 281.7502 - val_loss: 262.9498\n",
            "\n",
            "Epoch 19/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 279.6755 - val_loss: 263.6507\n",
            "\n",
            "Epoch 20/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 279.5630 - val_loss: 272.7593\n",
            "\n",
            "Epoch 21/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 279.6100 - val_loss: 264.2388\n",
            "\n",
            "Epoch 22/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 276.8305 - val_loss: 264.7613\n",
            "\n",
            "Epoch 23/150\n",
            "783/783 [==============================]783/783 [==============================] - 341s 436ms/step - loss: 277.5427 - val_loss: 285.4884\n",
            "\n",
            "Epoch 24/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 276.4034 - val_loss: 285.2003\n",
            "\n",
            "Epoch 25/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 276.5034 - val_loss: 263.7201\n",
            "\n",
            "Epoch 26/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 276.4264 - val_loss: 263.5141\n",
            "\n",
            "Epoch 27/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 275.5968 - val_loss: 261.4074\n",
            "\n",
            "Epoch 28/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 275.4084 - val_loss: 261.8558\n",
            "\n",
            "Epoch 29/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 436ms/step - loss: 276.3437 - val_loss: 269.8358\n",
            "\n",
            "Epoch 30/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 436ms/step - loss: 275.2132 - val_loss: 262.1449\n",
            "\n",
            "Epoch 31/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 272.2072 - val_loss: 264.4377\n",
            "\n",
            "Epoch 32/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 266.1467 - val_loss: 270.9474\n",
            "\n",
            "Epoch 33/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 264.8689 - val_loss: 291.7394\n",
            "\n",
            "Epoch 34/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 263.4361 - val_loss: 261.0260\n",
            "\n",
            "Epoch 35/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 262.3452 - val_loss: 247.1435\n",
            "\n",
            "Epoch 36/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 261.3327 - val_loss: 248.9725\n",
            "\n",
            "Epoch 37/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 262.3321 - val_loss: 251.0357\n",
            "\n",
            "Epoch 38/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 436ms/step - loss: 262.4982 - val_loss: 263.7822\n",
            "\n",
            "Epoch 39/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 261.0347 - val_loss: 263.7236\n",
            "\n",
            "Epoch 40/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 436ms/step - loss: 260.7313 - val_loss: 249.2669\n",
            "\n",
            "Epoch 41/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 436ms/step - loss: 260.0052 - val_loss: 246.7453\n",
            "\n",
            "Epoch 42/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 436ms/step - loss: 259.0272 - val_loss: 252.4105\n",
            "\n",
            "Epoch 43/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 436ms/step - loss: 259.1369 - val_loss: 302.8138\n",
            "\n",
            "Epoch 44/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 436ms/step - loss: 260.7136 - val_loss: 255.7194\n",
            "\n",
            "Epoch 45/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 258.5836 - val_loss: 251.4104\n",
            "\n",
            "Epoch 46/150\n",
            "783/783 [==============================]783/783 [==============================] - 341s 436ms/step - loss: 260.3360 - val_loss: 253.1794\n",
            "\n",
            "Epoch 47/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 436ms/step - loss: 258.7364 - val_loss: 245.6132\n",
            "\n",
            "Epoch 48/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 258.9119 - val_loss: 247.2452\n",
            "\n",
            "Epoch 49/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 263.7690 - val_loss: 253.3919\n",
            "\n",
            "Epoch 50/150\n",
            "783/783 [==============================]783/783 [==============================] - 345s 440ms/step - loss: 261.0069 - val_loss: 252.1991\n",
            "\n",
            "Epoch 51/150\n",
            "783/783 [==============================]783/783 [==============================] - 344s 440ms/step - loss: 260.1194 - val_loss: 251.5349\n",
            "\n",
            "Epoch 52/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 258.0237 - val_loss: 424.9160\n",
            "\n",
            "Epoch 53/150\n",
            "783/783 [==============================]783/783 [==============================] - 341s 436ms/step - loss: 260.4332 - val_loss: 251.1810\n",
            "\n",
            "Epoch 54/150\n",
            "783/783 [==============================]783/783 [==============================] - 341s 436ms/step - loss: 258.4968 - val_loss: 251.1531\n",
            "\n",
            "Epoch 55/150\n",
            "783/783 [==============================]783/783 [==============================] - 341s 436ms/step - loss: 258.8291 - val_loss: 244.4987\n",
            "\n",
            "Epoch 56/150\n",
            "783/783 [==============================]783/783 [==============================] - 341s 435ms/step - loss: 257.6883 - val_loss: 243.4465\n",
            "\n",
            "Epoch 57/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 259.3620 - val_loss: 244.1895\n",
            "\n",
            "Epoch 58/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 257.3870 - val_loss: 246.1976\n",
            "\n",
            "Epoch 59/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 256.5183 - val_loss: 251.1354\n",
            "\n",
            "Epoch 60/150\n",
            "783/783 [==============================]783/783 [==============================] - 344s 439ms/step - loss: 256.7974 - val_loss: 261.2650\n",
            "\n",
            "Epoch 61/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 439ms/step - loss: 256.3967 - val_loss: 244.5513\n",
            "\n",
            "Epoch 62/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 256.1617 - val_loss: 245.0334\n",
            "\n",
            "Epoch 63/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 256.1304 - val_loss: 243.2764\n",
            "\n",
            "Epoch 64/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 256.2005 - val_loss: 242.5592\n",
            "\n",
            "Epoch 65/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 256.4473 - val_loss: 256.5721\n",
            "\n",
            "Epoch 66/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 256.1685 - val_loss: 247.3742\n",
            "\n",
            "Epoch 67/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 439ms/step - loss: 254.3732 - val_loss: 249.5707\n",
            "\n",
            "Epoch 68/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 254.8626 - val_loss: 241.2910\n",
            "\n",
            "Epoch 69/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 258.9391 - val_loss: 246.9690\n",
            "\n",
            "Epoch 70/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 256.8046 - val_loss: 274.2502\n",
            "\n",
            "Epoch 71/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 258.8429 - val_loss: 247.4149\n",
            "\n",
            "Epoch 72/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 257.6116 - val_loss: 253.6774\n",
            "\n",
            "Epoch 73/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 254.8512 - val_loss: 242.1679\n",
            "\n",
            "Epoch 74/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 437ms/step - loss: 255.2229 - val_loss: 258.9456\n",
            "\n",
            "Epoch 75/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 255.1711 - val_loss: 246.0924\n",
            "\n",
            "Epoch 76/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 254.6018 - val_loss: 242.6300\n",
            "\n",
            "Epoch 77/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 255.1642 - val_loss: 240.7758\n",
            "\n",
            "Epoch 78/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 253.8957 - val_loss: 248.1797\n",
            "\n",
            "Epoch 79/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 256.7446 - val_loss: 247.3442\n",
            "\n",
            "Epoch 80/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 436ms/step - loss: 258.2414 - val_loss: 254.4469\n",
            "\n",
            "Epoch 81/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 255.0371 - val_loss: 255.9964\n",
            "\n",
            "Epoch 82/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 436ms/step - loss: 255.7554 - val_loss: 247.2628\n",
            "\n",
            "Epoch 83/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 436ms/step - loss: 254.2924 - val_loss: 240.0509\n",
            "\n",
            "Epoch 84/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 436ms/step - loss: 253.6690 - val_loss: 249.4463\n",
            "\n",
            "Epoch 85/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 436ms/step - loss: 257.0931 - val_loss: 246.7256\n",
            "\n",
            "Epoch 86/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 436ms/step - loss: 252.9901 - val_loss: 252.1988\n",
            "\n",
            "Epoch 87/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 436ms/step - loss: 254.1542 - val_loss: 240.2222\n",
            "\n",
            "Epoch 88/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 253.0485 - val_loss: 242.2843\n",
            "\n",
            "Epoch 89/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 253.8203 - val_loss: 242.3577\n",
            "\n",
            "Epoch 90/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 437ms/step - loss: 256.3868 - val_loss: 247.2423\n",
            "\n",
            "Epoch 91/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 252.7839 - val_loss: 241.7574\n",
            "\n",
            "Epoch 92/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 251.5494 - val_loss: 243.4760\n",
            "\n",
            "Epoch 93/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 252.3683 - val_loss: 248.9383\n",
            "\n",
            "Epoch 94/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 252.4432 - val_loss: 238.4124\n",
            "\n",
            "Epoch 95/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 252.0509 - val_loss: 239.1572\n",
            "\n",
            "Epoch 96/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 437ms/step - loss: 251.6546 - val_loss: 240.5408\n",
            "\n",
            "Epoch 97/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 437ms/step - loss: 251.7691 - val_loss: 238.7595\n",
            "\n",
            "Epoch 98/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 251.4472 - val_loss: 245.0573\n",
            "\n",
            "Epoch 99/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 251.6537 - val_loss: 238.2036\n",
            "\n",
            "Epoch 100/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 251.4427 - val_loss: 242.2943\n",
            "\n",
            "Epoch 101/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 252.3212 - val_loss: 244.2949\n",
            "\n",
            "Epoch 102/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 251.2896 - val_loss: 240.4660\n",
            "\n",
            "Epoch 103/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 437ms/step - loss: 250.9906 - val_loss: 238.4749\n",
            "\n",
            "Epoch 104/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 437ms/step - loss: 252.6563 - val_loss: 241.3288\n",
            "\n",
            "Epoch 105/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 437ms/step - loss: 250.0704 - val_loss: 237.8188\n",
            "\n",
            "Epoch 106/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 251.0596 - val_loss: 242.1031\n",
            "\n",
            "Epoch 107/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 249.9701 - val_loss: 237.7694\n",
            "\n",
            "Epoch 108/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 250.3092 - val_loss: 256.2376\n",
            "\n",
            "Epoch 109/150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 251.3499 - val_loss: 249.3952\n",
            "\n",
            "Epoch 110/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 250.2844 - val_loss: 246.8159\n",
            "\n",
            "Epoch 111/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 249.9835 - val_loss: 241.4105\n",
            "\n",
            "Epoch 112/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 250.9328 - val_loss: 238.3921\n",
            "\n",
            "Epoch 113/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 249.2512 - val_loss: 255.4767\n",
            "\n",
            "Epoch 114/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 250.3800 - val_loss: 242.5440\n",
            "\n",
            "Epoch 115/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 250.3569 - val_loss: 247.4694\n",
            "\n",
            "Epoch 116/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 249.9381 - val_loss: 239.2597\n",
            "\n",
            "Epoch 117/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 437ms/step - loss: 249.3863 - val_loss: 237.6421\n",
            "\n",
            "Epoch 118/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 249.4240 - val_loss: 241.9237\n",
            "\n",
            "Epoch 119/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 249.2908 - val_loss: 241.7404\n",
            "\n",
            "Epoch 120/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 437ms/step - loss: 250.1710 - val_loss: 236.1869\n",
            "\n",
            "Epoch 121/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 438ms/step - loss: 250.0534 - val_loss: 254.1226\n",
            "\n",
            "Epoch 122/150\n",
            "783/783 [==============================]783/783 [==============================] - 343s 437ms/step - loss: 250.6064 - val_loss: 238.7536\n",
            "\n",
            "Epoch 123/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 248.8168 - val_loss: 251.3573\n",
            "\n",
            "Epoch 124/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 251.3285 - val_loss: 240.8012\n",
            "\n",
            "Epoch 125/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 249.6438 - val_loss: 239.0513\n",
            "\n",
            "Epoch 126/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 249.1690 - val_loss: 247.4387\n",
            "\n",
            "Epoch 127/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 254.5499 - val_loss: 246.4069\n",
            "\n",
            "Epoch 128/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 247.8308 - val_loss: 242.3180\n",
            "\n",
            "Epoch 129/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 248.5105 - val_loss: 236.8944\n",
            "\n",
            "Epoch 130/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 248.6724 - val_loss: 284.4524\n",
            "\n",
            "Epoch 131/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 249.8260 - val_loss: 244.1053\n",
            "\n",
            "Epoch 132/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 247.7358 - val_loss: 240.6808\n",
            "\n",
            "Epoch 133/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 248.7069 - val_loss: 235.6119\n",
            "\n",
            "Epoch 134/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 249.6713 - val_loss: 237.4062\n",
            "\n",
            "Epoch 135/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 249.8465 - val_loss: 265.5299\n",
            "\n",
            "Epoch 136/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 253.5427 - val_loss: 240.5963\n",
            "\n",
            "Epoch 137/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 247.8229 - val_loss: 235.7135\n",
            "\n",
            "Epoch 138/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 247.7467 - val_loss: 245.2391\n",
            "\n",
            "Epoch 139/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 248.8080 - val_loss: 238.8339\n",
            "\n",
            "Epoch 140/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 247.9371 - val_loss: 237.3430\n",
            "\n",
            "Epoch 141/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 247.2363 - val_loss: 237.4085\n",
            "\n",
            "Epoch 142/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 247.9702 - val_loss: 235.9035\n",
            "\n",
            "Epoch 143/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 246.7989 - val_loss: 241.0476\n",
            "\n",
            "Epoch 144/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 247.6706 - val_loss: 238.5947\n",
            "\n",
            "Epoch 145/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 247.2723 - val_loss: 245.9259\n",
            "\n",
            "Epoch 146/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 248.1925 - val_loss: 241.1973\n",
            "\n",
            "Epoch 147/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 436ms/step - loss: 246.9683 - val_loss: 244.7321\n",
            "\n",
            "Epoch 148/150\n",
            "783/783 [==============================]783/783 [==============================] - 340s 434ms/step - loss: 247.5598 - val_loss: 252.8552\n",
            "\n",
            "Epoch 149/150\n",
            "783/783 [==============================]783/783 [==============================] - 342s 437ms/step - loss: 247.0924 - val_loss: 238.7884\n",
            "\n",
            "Epoch 150/150\n",
            "783/783 [==============================]783/783 [==============================] - 341s 436ms/step - loss: 247.8885 - val_loss: 246.2385\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras._impl.keras.callbacks.History at 0x7f746a340320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "ZgTkTgniRxpu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### モデルの予測"
      ]
    },
    {
      "metadata": {
        "id": "l9MwL9g3Rxpw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "preds = autoencoder.predict_generator(test_gen, steps=test_steps, verbose=0)\n",
        "\n",
        "x_test = []\n",
        "y_test = []\n",
        "for i, (l, ab) in enumerate(generator_with_preprocessing(test_lists, batch_size)):\n",
        "    x_test.append(l)\n",
        "    y_test.append(ab)\n",
        "    if i == (test_steps - 1):\n",
        "        break\n",
        "        \n",
        "x_test = np.vstack(x_test)\n",
        "y_test = np.vstack(y_test)\n",
        "\n",
        "test_preds_lab = np.concatenate((x_test, preds), 3).astype(np.uint8)\n",
        "\n",
        "test_preds_rgb = []\n",
        "for i in range(test_preds_lab.shape[0]):\n",
        "    preds_rgb = lab2rgb(test_preds_lab[i, :, :, :])\n",
        "    test_preds_rgb.append(preds_rgb)\n",
        "test_preds_rgb = np.stack(test_preds_rgb)\n",
        "\n",
        "from IPython.display import display_png\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "\n",
        "for i in range(test_preds_rgb.shape[0]):\n",
        "    gray_image = ImageOps.grayscale(array_to_img(test_preds_rgb[i]))\n",
        "    display_png(gray_image)\n",
        "    display_png(array_to_img(test_preds_rgb[i]))\n",
        "    print('-'*25)\n",
        "    if i == 20:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}